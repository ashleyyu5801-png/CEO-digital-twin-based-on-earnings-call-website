{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import html\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder = \"../Results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. General Similarity\n",
    "\n",
    "- Model: ProsusAI FinBERT (A Sentence BERT model further trained on financial text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name ProsusAI/finbert. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f3a50f943c14ef58b33d9b23a0fbbd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6382304c0e48e691a06663bfecf0a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/252 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc00a9eba685414aadfa42a7ed1dce83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba6a10c5b0f41f2bfeb2231e16056bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: ProsusAI/finbert\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"ProsusAI/finbert\"\n",
    "model = SentenceTransformer(model_name)\n",
    "print(f\"Loaded model: {model_name}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: spec1_random_500_20251110_180802.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing similarities for spec1_random_500_20251110_180802.json: 100%|██████████| 43/43 [00:03<00:00, 13.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved per-sample results to: ../Results/Evaluation/SBERT/spec1_random_500_20251110_180802_similarity.csv\n",
      "Processing file: spec2_recent_500_20251110_181330.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing similarities for spec2_recent_500_20251110_181330.json: 100%|██████████| 43/43 [00:03<00:00, 13.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved per-sample results to: ../Results/Evaluation/SBERT/spec2_recent_500_20251110_181330_similarity.csv\n",
      "Processing file: spec3_recent_500_plus_10q_20251110_182010.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing similarities for spec3_recent_500_plus_10q_20251110_182010.json: 100%|██████████| 43/43 [00:03<00:00, 13.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved per-sample results to: ../Results/Evaluation/SBERT/spec3_recent_500_plus_10q_20251110_182010_similarity.csv\n",
      "Processing file: spec4_persona_plus_10q_20251110_182428.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing similarities for spec4_persona_plus_10q_20251110_182428.json: 100%|██████████| 43/43 [00:03<00:00, 12.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved per-sample results to: ../Results/Evaluation/SBERT/spec4_persona_plus_10q_20251110_182428_similarity.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "summary_SBERT = []\n",
    "\n",
    "evaluation_SBERT_folder = \"../Results/Evaluation/SBERT\"\n",
    "\n",
    "\n",
    "for file_name in sorted(os.listdir(results_folder)):\n",
    "    if not file_name.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(results_folder, file_name)\n",
    "    print(f\"Processing file: {file_name}\")\n",
    "\n",
    "    # Load the JSON structure\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        file_data = json.load(f)\n",
    "\n",
    "    results = file_data.get(\"results\", [])\n",
    "    if not results:\n",
    "        print(f\"No results found in {file_name}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Compute cosine similarities\n",
    "    similarities = []\n",
    "    for r in tqdm(results, desc=f\"Computing similarities for {file_name}\"):\n",
    "        gt = r.get(\"ground_truth_answer\", \"\").strip()\n",
    "        gen = r.get(\"generated_answer\", \"\").strip()\n",
    "\n",
    "        if not gt or not gen:\n",
    "            sim = None\n",
    "        else:\n",
    "            emb_gt = model.encode(gt, convert_to_tensor=True)\n",
    "            emb_gen = model.encode(gen, convert_to_tensor=True)\n",
    "            sim = util.cos_sim(emb_gt, emb_gen).item()\n",
    "\n",
    "        r[\"similarity\"] = round(sim, 4) if sim is not None else None\n",
    "        similarities.append(sim)\n",
    "\n",
    "    # Save per-file CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    output_csv = os.path.join(evaluation_SBERT_folder, file_name.replace(\".json\", \"_similarity.csv\"))\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Saved per-sample results to: {output_csv}\")\n",
    "\n",
    "    # Compute and store summary metrics\n",
    "    valid_sims = [s for s in similarities if s is not None]\n",
    "    avg_sim = sum(valid_sims) / len(valid_sims) if valid_sims else 0.0\n",
    "    summary_SBERT.append({\n",
    "        \"file\": file_name,\n",
    "        \"specification\": file_data.get(\"specification\", \"\"),\n",
    "        \"description\": file_data.get(\"description\", \"\"),\n",
    "        \"samples\": len(valid_sims),\n",
    "        \"average_similarity\": round(avg_sim, 4)\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary saved to ../Results/Evaluation/SBERT/SBERT.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if summary_SBERT:\n",
    "    summary_SBERT_df = pd.DataFrame(summary_SBERT).sort_values(by=\"average_similarity\", ascending=False)\n",
    "    summary_SBERT_df_csv = os.path.join(evaluation_SBERT_folder, \"SBERT.csv\")\n",
    "    summary_SBERT_df.to_csv(summary_SBERT_df_csv, index=False)\n",
    "    print(f\"Summary saved to {summary_SBERT_df_csv}\\n\")\n",
    "else:\n",
    "    print(\"No valid JSON result files found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Informational Accuracy\n",
    "\n",
    "- Model: NLI (Natural Language Inference)\n",
    "\n",
    "- This evaluation measures informational accuracy through a bidirectional Natural Language Inference (NLI) process. For each question–answer pair, the model assesses the logical relationship between the generated answer and the ground truth in both directions. In the first direction, the generated answer is treated as the premise and the ground truth as the hypothesis, testing whether the generated response correctly entails the factual content of the reference. This captures **factual accuracy**, ensuring the model does not introduce contradictions or false information. In the reverse direction, the ground truth serves as the premise and the generated answer as the hypothesis, testing whether the model’s response covers all the essential information expressed in the reference. This second direction captures **completeness** or **coverage**. Averaging the entailment probabilities from both directions provides a balanced measure of informational accuracy, reflecting how well the generated answer aligns with and fully represents the reference answer’s meaning.\n",
    "\n",
    "### **Output Explanation**\n",
    "\n",
    "#### **1. Entailment**\n",
    "\n",
    "**gen → gt (Generated entails Ground Truth)**  \n",
    "This means the generated answer **supports or implies** what is stated in the ground truth.  \n",
    "A high entailment score in this direction shows that the generated answer is **factually accurate** — it correctly reflects or summarizes the truth provided in the reference.\n",
    "\n",
    "**gt → gen (Ground Truth entails Generated)**  \n",
    "This means the ground truth **covers or includes** what the generated answer says.  \n",
    "A high score here indicates the generated answer is **complete** — it does not omit key information from the ground truth.\n",
    "\n",
    "\n",
    "#### **2. Contradiction**\n",
    "\n",
    "**gen → gt**  \n",
    "A high contradiction score here indicates the generated answer **conflicts with the ground truth** — it says something that cannot be true if the ground truth is true.  \n",
    "This captures **factual errors** or **misleading statements** in the generated text.\n",
    "\n",
    "**gt → gen**  \n",
    "This means the ground truth **conflicts with what the generated answer says**.  \n",
    "A high score suggests the generated answer **omits or reverses facts** stated in the reference.\n",
    "\n",
    "\n",
    "#### **3. Neutral**\n",
    "\n",
    "**gen → gt**  \n",
    "A high neutral score here means the generated answer **neither confirms nor denies** the ground truth.  \n",
    "It might be too vague or general — the model can’t tell whether it’s right or wrong.  \n",
    "This usually signals **lack of specificity** or **partial coverage**.\n",
    "\n",
    "**gt → gen**  \n",
    "A high neutral score here means the ground truth contains information that the generated answer **does not address**.  \n",
    "This indicates **missing details** — the generated answer fails to cover factual statements made in the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b1e09c15694eae975763df49b9428a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a86492eb44f41a0abc50fcfc42cf096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cee8fd96fd0417d835071c1fb6f2cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f8862c3db8d4330ae9188b9d30198d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b437b0f86b647658246ba2fba22c3e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18602fdb54fc48c382f5c58b79b32b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = 0 if torch.cuda.is_available() else -1\n",
    "model_name = \"facebook/bart-large-mnli\"\n",
    "\n",
    "nli_pipe = pipeline(\"text-classification\", model=model_name, tokenizer=model_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating informational accuracy for: spec1_random_500_20251110_180802.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI scoring for spec1_random_500_20251110_180802.json: 100%|██████████| 43/43 [00:41<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved per-sample informational accuracy to: ../Results/Evaluation/NLI/spec1_random_500_20251110_180802_info_accuracy.csv\n",
      "\n",
      "Evaluating informational accuracy for: spec2_recent_500_20251110_181330.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI scoring for spec2_recent_500_20251110_181330.json: 100%|██████████| 43/43 [00:42<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved per-sample informational accuracy to: ../Results/Evaluation/NLI/spec2_recent_500_20251110_181330_info_accuracy.csv\n",
      "\n",
      "Evaluating informational accuracy for: spec3_recent_500_plus_10q_20251110_182010.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI scoring for spec3_recent_500_plus_10q_20251110_182010.json: 100%|██████████| 43/43 [00:45<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved per-sample informational accuracy to: ../Results/Evaluation/NLI/spec3_recent_500_plus_10q_20251110_182010_info_accuracy.csv\n",
      "\n",
      "Evaluating informational accuracy for: spec4_persona_plus_10q_20251110_182428.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NLI scoring for spec4_persona_plus_10q_20251110_182428.json: 100%|██████████| 43/43 [00:48<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved per-sample informational accuracy to: ../Results/Evaluation/NLI/spec4_persona_plus_10q_20251110_182428_info_accuracy.csv\n",
      "\n",
      "Saved summary informational accuracy to: ../Results/Evaluation/NLI/summary_informational_accuracy.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# Folder Setup\n",
    "# ---------------------------------------------------------------------\n",
    "results_folder = \"../Results\"\n",
    "evaluation_NLI_folder = \"../Results/Evaluation/NLI\"\n",
    "os.makedirs(evaluation_NLI_folder, exist_ok=True)\n",
    "\n",
    "summary_nli = []\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Helper: Extract label and score (single output)\n",
    "# ---------------------------------------------------------------------\n",
    "def extract_label_and_score(output):\n",
    "    \"\"\"\n",
    "    Extracts the label and score from the model output.\n",
    "    Example output: [{'label': 'neutral', 'score': 0.98}]\n",
    "    \"\"\"\n",
    "    if isinstance(output, list) and len(output) > 0:\n",
    "        item = output[0]\n",
    "        label = str(item.get(\"label\", \"\")).lower()\n",
    "        score = float(item.get(\"score\", 0.0))\n",
    "        return label, score\n",
    "    elif isinstance(output, dict):\n",
    "        label = str(output.get(\"label\", \"\")).lower()\n",
    "        score = float(output.get(\"score\", 0.0))\n",
    "        return label, score\n",
    "    return None, None\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Main Loop\n",
    "# ---------------------------------------------------------------------\n",
    "for file_name in sorted(os.listdir(results_folder)):\n",
    "    if not file_name.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(results_folder, file_name)\n",
    "    print(f\"\\nEvaluating informational accuracy for: {file_name}\")\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    results = data.get(\"results\", [])\n",
    "    entailment_scores = []\n",
    "    contradiction_scores = []\n",
    "    neutral_scores = []\n",
    "\n",
    "    for r in tqdm(results, desc=f\"NLI scoring for {file_name}\"):\n",
    "        gt = r.get(\"ground_truth_answer\", \"\").strip().replace(\"\\n\", \" \").replace(\"\\\"\", \"'\")\n",
    "        gen = r.get(\"generated_answer\", \"\").strip().replace(\"\\n\", \" \").replace(\"\\\"\", \"'\")\n",
    "\n",
    "        if not gt or not gen:\n",
    "            r[\"informational_accuracy\"] = None\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Bidirectional inference\n",
    "            out1 = nli_pipe({\"text\": gen, \"text_pair\": gt})\n",
    "            out2 = nli_pipe({\"text\": gt, \"text_pair\": gen})\n",
    "\n",
    "            label1, score1 = extract_label_and_score(out1)\n",
    "            label2, score2 = extract_label_and_score(out2)\n",
    "\n",
    "            # Store both directions\n",
    "            r[\"gen_to_gt_label\"] = label1\n",
    "            r[\"gen_to_gt_score\"] = score1\n",
    "            r[\"gt_to_gen_label\"] = label2\n",
    "            r[\"gt_to_gen_score\"] = score2\n",
    "\n",
    "\n",
    "            # Collect per-type scores for summary\n",
    "            if label1 == \"entailment\":\n",
    "                entailment_scores.append(score1)\n",
    "            elif label1 == \"contradiction\":\n",
    "                contradiction_scores.append(score1)\n",
    "            elif label1 == \"neutral\":\n",
    "                neutral_scores.append(score1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error for sample: {e}\")\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Save Per-File Results\n",
    "    # -----------------------------------------------------------------\n",
    "    df = pd.DataFrame(results)\n",
    "    out_csv = os.path.join(evaluation_NLI_folder, file_name.replace(\".json\", \"_info_accuracy.csv\"))\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved per-sample informational accuracy to: {out_csv}\")\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Summary Statistics\n",
    "    # -----------------------------------------------------------------\n",
    "    avg_entail = sum(entailment_scores) / len(entailment_scores) if entailment_scores else 0.0\n",
    "    avg_contradiction = sum(contradiction_scores) / len(contradiction_scores) if contradiction_scores else 0.0\n",
    "    avg_neutral = sum(neutral_scores) / len(neutral_scores) if neutral_scores else 0.0\n",
    "    samples = len(results)\n",
    "\n",
    "    summary_nli.append({\n",
    "        \"file\": file_name,\n",
    "        \"specification\": data.get(\"specification\", \"\"),\n",
    "        \"samples\": samples,\n",
    "        \"avg_entail_score\": round(avg_entail, 4),\n",
    "        \"avg_contradiction_score\": round(avg_contradiction, 4),\n",
    "        \"avg_neutral_score\": round(avg_neutral, 4),\n",
    "    })\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Save Summary\n",
    "# ---------------------------------------------------------------------\n",
    "summary_NLI_df = pd.DataFrame(summary_nli).sort_values(by=\"avg_entail_score\", ascending=False)\n",
    "summary_NLI_csv = os.path.join(evaluation_NLI_folder, \"summary_informational_accuracy.csv\")\n",
    "summary_NLI_df.to_csv(summary_NLI_csv, index=False)\n",
    "\n",
    "print(f\"\\nSaved summary informational accuracy to: {summary_NLI_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tone Alignment\n",
    "\n",
    "This code evaluates **tone alignment** between generated and ground-truth answers using the **Loughran & McDonald (2011)** financial sentiment framework.\n",
    "\n",
    "It combines **FinBERT**’s model-based tone detection (for *positive*, *neutral*, and *negative*) with a **lexicon-based approach** that identifies additional tones — *uncertainty*, *litigious*, *constraining*, and *modal* — using keyword frequencies from the L&M dictionary.\n",
    "\n",
    "For each answer pair, it creates a **7-dimensional tone vector** representing these categories, then calculates the **cosine similarity** between the generated and ground-truth vectors to measure how closely their tones align.\n",
    "\n",
    "The script outputs detailed per-sample tone labels and scores, along with a summary file reporting the **average tone alignment** for each dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Folder Setup\n",
    "# --------------------------------------------------------------\n",
    "results_folder = \"../Results\"\n",
    "evaluation_tone_folder = \"../Results/Evaluation/Tone\"\n",
    "os.makedirs(evaluation_tone_folder, exist_ok=True)\n",
    "\n",
    "summary_tone = []\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Load FinBERT (Positive / Neutral / Negative)\n",
    "# --------------------------------------------------------------\n",
    "model_name = \"yiyanghkust/finbert-tone\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "finbert_labels = [\"positive\", \"neutral\", \"negative\"]\n",
    "\n",
    "def get_finbert_probs(text):\n",
    "    \"\"\"Return normalized FinBERT sentiment probabilities for a given text.\"\"\"\n",
    "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "        return np.array([0.0, 0.0, 0.0])\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=1).numpy()[0]\n",
    "    return probs\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Loughran & McDonald Lexicon Extensions\n",
    "# --------------------------------------------------------------\n",
    "lm_lexicons = {\n",
    "    \"uncertainty\": [\n",
    "        \"uncertain\", \"unsure\", \"risk\", \"doubt\", \"fluctuate\", \"potential\", \"contingent\",\n",
    "        \"approximately\", \"maybe\", \"possible\", \"possibly\", \"likely\", \"unlikely\"\n",
    "    ],\n",
    "    \"litigious\": [\n",
    "        \"claim\", \"lawsuit\", \"litigation\", \"sue\", \"settlement\", \"regulation\", \"penalty\",\n",
    "        \"court\", \"compliance\", \"dispute\", \"infringement\"\n",
    "    ],\n",
    "    \"constraining\": [\n",
    "        \"restrict\", \"limit\", \"bound\", \"constraint\", \"mandatory\", \"require\", \"prohibit\",\n",
    "        \"ban\", \"obligation\", \"restrictive\"\n",
    "    ],\n",
    "    \"modal\": [\n",
    "        \"should\", \"could\", \"would\", \"might\", \"may\", \"can\", \"must\", \"shall\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def lexicon_tone_score(text):\n",
    "    \"\"\"Compute frequency-based scores for additional tone categories.\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    scores = []\n",
    "    for cat, words in lm_lexicons.items():\n",
    "        count = sum(len(re.findall(rf\"\\b{w}\\b\", text_lower)) for w in words)\n",
    "        scores.append(count)\n",
    "    # normalize to [0, 1]\n",
    "    total = sum(scores) if sum(scores) > 0 else 1\n",
    "    return np.array([s / total for s in scores])\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Combine FinBERT + Lexicon into 7-Dim Tone Vector\n",
    "# --------------------------------------------------------------\n",
    "def get_tone_vector(text):\n",
    "    finbert_probs = get_finbert_probs(text)  # 3 dims\n",
    "    lexicon_scores = lexicon_tone_score(text)  # 4 dims\n",
    "    combined = np.concatenate([finbert_probs, lexicon_scores])  # total 7 dims\n",
    "    # normalize the whole vector\n",
    "    if np.sum(combined) > 0:\n",
    "        combined = combined / np.sum(combined)\n",
    "    return combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 56e12f19-5c0f-4837-83df-9299a175e1af)')' thrown while requesting HEAD https://huggingface.co/yiyanghkust/finbert-tone/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c93943ec5c4475a9b9663498dc924e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac8738c6f69b44678787cc85f103d0cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/439M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating tone alignment for: spec1_random_500_20251110_180802.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tone alignment for spec1_random_500_20251110_180802.json:   0%|          | 0/43 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for sample: The size of tensor a (583) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tone alignment for spec1_random_500_20251110_180802.json: 100%|██████████| 43/43 [00:06<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved per-sample tone alignment to: ../Results/Evaluation/Tone/spec1_random_500_20251110_180802_tone_alignment.csv\n",
      "\n",
      "Evaluating tone alignment for: spec2_recent_500_20251110_181330.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tone alignment for spec2_recent_500_20251110_181330.json:   5%|▍         | 2/43 [00:00<00:03, 13.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for sample: The size of tensor a (583) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tone alignment for spec2_recent_500_20251110_181330.json: 100%|██████████| 43/43 [00:06<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved per-sample tone alignment to: ../Results/Evaluation/Tone/spec2_recent_500_20251110_181330_tone_alignment.csv\n",
      "\n",
      "Evaluating tone alignment for: spec3_recent_500_plus_10q_20251110_182010.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tone alignment for spec3_recent_500_plus_10q_20251110_182010.json:   5%|▍         | 2/43 [00:00<00:02, 13.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for sample: The size of tensor a (583) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tone alignment for spec3_recent_500_plus_10q_20251110_182010.json: 100%|██████████| 43/43 [00:06<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved per-sample tone alignment to: ../Results/Evaluation/Tone/spec3_recent_500_plus_10q_20251110_182010_tone_alignment.csv\n",
      "\n",
      "Evaluating tone alignment for: spec4_persona_plus_10q_20251110_182428.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tone alignment for spec4_persona_plus_10q_20251110_182428.json:   5%|▍         | 2/43 [00:00<00:03, 11.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for sample: The size of tensor a (583) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tone alignment for spec4_persona_plus_10q_20251110_182428.json:  98%|█████████▊| 42/43 [00:06<00:00,  6.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for sample: The size of tensor a (628) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tone alignment for spec4_persona_plus_10q_20251110_182428.json: 100%|██████████| 43/43 [00:06<00:00,  6.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved per-sample tone alignment to: ../Results/Evaluation/Tone/spec4_persona_plus_10q_20251110_182428_tone_alignment.csv\n",
      "\n",
      "Saved summary tone alignment to: ../Results/Evaluation/Tone/summary_tone_alignment.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Process Each JSON File\n",
    "# --------------------------------------------------------------\n",
    "for file_name in sorted(os.listdir(results_folder)):\n",
    "    if not file_name.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(results_folder, file_name)\n",
    "    print(f\"\\nEvaluating tone alignment for: {file_name}\")\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    results = data.get(\"results\", [])\n",
    "    tone_scores = []\n",
    "\n",
    "    for r in tqdm(results, desc=f\"Tone alignment for {file_name}\"):\n",
    "        gt = r.get(\"ground_truth_answer\", \"\").strip().replace(\"\\n\", \" \").replace(\"\\\"\", \"'\")\n",
    "        gen = r.get(\"generated_answer\", \"\").strip().replace(\"\\n\", \" \").replace(\"\\\"\", \"'\")\n",
    "\n",
    "        if not gt or not gen:\n",
    "            r[\"tone_alignment\"] = None\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Get 7-dim tone probability vectors\n",
    "            gt_tone = get_tone_vector(gt)\n",
    "            gen_tone = get_tone_vector(gen)\n",
    "\n",
    "            # Compute cosine similarity\n",
    "            tone_score = float(cosine_similarity([gt_tone], [gen_tone])[0][0])\n",
    "\n",
    "            # Record top tone categories\n",
    "            labels_7 = [\"positive\", \"neutral\", \"negative\", \"uncertainty\", \"litigious\", \"constraining\", \"modal\"]\n",
    "            r[\"tone_alignment\"] = round(tone_score, 4)\n",
    "            r[\"gt_tone_label\"] = labels_7[int(np.argmax(gt_tone))]\n",
    "            r[\"gen_tone_label\"] = labels_7[int(np.argmax(gen_tone))]\n",
    "\n",
    "            tone_scores.append(tone_score)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error for sample: {e}\")\n",
    "            r[\"tone_alignment\"] = None\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Save Per-File Results\n",
    "    # ----------------------------------------------------------\n",
    "    df = pd.DataFrame(results)\n",
    "    out_csv = os.path.join(evaluation_tone_folder, file_name.replace(\".json\", \"_tone_alignment.csv\"))\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved per-sample tone alignment to: {out_csv}\")\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Summary Statistics\n",
    "    # ----------------------------------------------------------\n",
    "    valid_scores = [s for s in tone_scores if s is not None]\n",
    "    avg_score = sum(valid_scores) / len(valid_scores) if valid_scores else 0.0\n",
    "\n",
    "    summary_tone.append({\n",
    "        \"file\": file_name,\n",
    "        \"specification\": data.get(\"specification\", \"\"),\n",
    "        \"samples\": len(valid_scores),\n",
    "        \"avg_tone_alignment\": round(avg_score, 4)\n",
    "    })\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Save Summary Across All Files\n",
    "# --------------------------------------------------------------\n",
    "summary_tone_df = pd.DataFrame(summary_tone).sort_values(by=\"avg_tone_alignment\", ascending=False)\n",
    "summary_tone_csv = os.path.join(evaluation_tone_folder, \"summary_tone_alignment.csv\")\n",
    "summary_tone_df.to_csv(summary_tone_csv, index=False)\n",
    "\n",
    "print(f\"\\nSaved summary tone alignment to: {summary_tone_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
